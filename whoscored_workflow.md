# WhoScored Minuteâ€‘byâ€‘Minute ScrapingÂ â€“ **Updated Python Workflow**

> **Audience**  Junior data engineer familiar with Python but new to webâ€‘scraping & basic databases.
> **Goal**  Build a reproducible pipeline that (1) discovers fixtures for any league, (2) scrapes minuteâ€‘level match data, and (3) persists both metadata & granular stats to an SQLite dataâ€‘mart ready for analysis or export to BI tools.

---

## 0Â Â Project Setup

1. **Create a fresh virtual environment** (e.g. `python -m venv ws_env`) and install core packages:

   * Web requestsÂ & parsing â€” `requests`, `beautifulsoup4`, `lxml`, `cloudscraper`
   * Browser automation â€” `selenium`, `undetectedâ€‘chromedriver`
   * Data wrangling â€” `pandas`, `pyarrow`, `tenacity`
   * Database layer â€” `SQLAlchemy`, `sqlite-utils` (optional CLI helpers)
   * ConfigÂ & secrets â€” `pythonâ€‘dotenv`
2. **Repo skeleton**

   ```text
   whoscored_scraper/
   â”œâ”€â”€ data/                # Raw JSON, interim parquet
   â”œâ”€â”€ ws/                  # Package code
   â”‚Â Â  â”œâ”€â”€ db.py            # SQLite models & helpers
   â”‚Â Â  â”œâ”€â”€ fixtures.py      # Leagueâ€‘level fixture scraper (uses Selenium)
   â”‚Â Â  â”œâ”€â”€ match.py         # Matchâ€‘level scrapers (JSON or HTML route)
   â”‚Â Â  â”œâ”€â”€ parse.py         # JSONÂ â†’ tidy DataFrames
   â”‚Â Â  â””â”€â”€ cli.py           # Entryâ€‘point & orchestration
   â””â”€â”€ README.md
   ```

---

## 1Â Â Discover Fixtures (**new module**)

### 1.1  Purpose

Automate collection of **match / fixture IDs** for a selected league & date window (default Â±Â 3Â months either side of today).

### 1.2  Implementation highlights (`ws/fixtures.py`)

1. Accept a **league overview URL** (e.g. PremierÂ League 2024â€‘25 fixtures page).
2. Spin up **undetectedâ€‘Chrome** â wait for calendar component.
3. Iterate **Prev / Next** month buttons up to *NÂ months*; collect all `<a href="â€¦/matches/{id}/â€¦">` links.
4. Deâ€‘duplicate IDs, return `List[int]`.
5. Optionally write to `data/raw/{league}_{yyyymmdd}.fixtures.json`.

> A working prototype lives in `fetch_epl_fixtures.py`; refactor its logic into a reusable `get_fixture_ids(url:str, months:int)->List[int]`.

---

## 2Â Â Database Layer (**new module**)

### 2.1  Why SQLite?

* Zeroâ€‘config, crossâ€‘platform, perfect for <Â GBâ€‘scale analytics.
* Schema versioned inâ€‘repo; easily promoted to Postgres later.

### 2.2  Schema (ERâ€‘style overview)

```
competitions(id PK, name, country, season, stage, scraped_at)
fixtures(id PK, competition_id FK, date_utc, home_team, away_team,
         round, referee, venue, scraped_at)
minutes(match_id FK, minute PK, added_time, possession_home, possession_away,
        rating_home, rating_away, total_shots_home, total_shots_away, pass_success_home, pass_success_away, dribbles_home, dribbles_away, aerial_won_home, aerial_won_away, tackles_home, tackles_away, corners_home, corners_away, scraped_at)
```

*All tables share `scraped_at` for lineage.*

### 2.3  Helper API (`ws/db.py`)

* `get_engine(db_path="data/ws.db")` â†’ returns SQLAlchemy engine.
* `upsert(table:str, df:pd.DataFrame, pk:list[str])` â†’ generic UPSERT (handles duplicates).
* `fixture_exists(match_id:int)` â bool (skip reâ€‘scrape).

---

## 3Â Â Identify Target Matches

1. **ModeÂ A â€“ From fixture scraper**

   ```python
   ids = fixtures.get_fixture_ids(league_url, months=6)
   ```
2. **ModeÂ B â€“ Adâ€‘hoc list** passed at CLI (`--match 1825717 1825720`).
3. Filter out IDs already present in `fixtures` table unless `--force` flag is set.

---

## 4Â Â Handle Antiâ€‘Bot & Session Reâ€‘use

*Unchanged from earlier doc â€“ Cloudscraper first, Selenium fallback, cookie reuse.*

---

## 5Â Â Matchâ€‘Level Data Acquisition

### 5.1  Public JSON endpoints

Same `/MatchEvents`, `/teamstatistics`, `/playerRatingGraph` flow.  Raw JSON persisted to `data/raw/`.

### 5.2  Fallback â€“ `matchCentreData` script scraping

Use `ws/match.py:fetch_match_centre_data(match_id)` (ported from `proto.py`).

---

## 6Â Â Parsing & Normalisation (`ws/parse.py`)

Functions convert raw payloads to tidy DataFrames ready for DB insert.  *Reuse logic from the original workflow.*

---

## 7Â Â Persistence to SQLite (**updated section**)

```python
engine = db.get_engine()

# â¬‡ write granular minuteâ€‘graph
minutes_df.to_sql("minutes", engine, if_exists="append", index=False)

# â¬‡ write oneâ€‘row aggregate per match/team
agg_df.to_sql("agg_stats", engine, if_exists="append", index=False)
```

* Use `upsert()` to prevent duplicates when reâ€‘running.
* Wrap all inserts in a single transaction for speed.

---

## 8Â Â Postâ€‘Processing & Quality Control

1. **DBâ€‘centric checks** â€“ e.g. `SELECT COUNT(*)Â«` minutes covering 0â€‘90.
2. Flag possession sums â‰  100.
3. Create **materialised views** for common queries (e.g. perâ€‘minute xG once added).

---

## 9Â Â CLI & Automation

```bash
python -m ws.cli scrape \
  --league "https://www.whoscored.com/regions/252/.../fixtures" \
  --months 6 \
  --db data/ws.db \
  --output parquet
```

**Command groups**

| Command      | Action                                            |
| ------------ | ------------------------------------------------- |
| `scrape`     | Discover fixtures â†’ scrape â†’ load DB              |
| `refresh`    | Rescrape last *N* days (to catch postponed games) |
| `export csv` | Dump tables/queries for BI tools                  |

Set up a **cron / Task Scheduler** job nightly; pipe logs to a file.

---

## 10Â Â Error Handling & Edge Cases

| Scenario                       | Strategy                      |
| ------------------------------ | ----------------------------- |
| Fixture calendar fails to load | Retry with fullâ€‘browser UA âœ”  |
| DB lock (another job running)  | Backâ€‘off & retry after 60â€¯s   |
| Upsert violates schema change  | Migrate DB via Alembic script |

---

## 11Â Â Ethical & Legal Notes

Same as before **plus**: storing data locally does not grant redistribution rights; check WhoScoredÂ T\&C before sharing DB dumps.

---

## 12Â Â Next Steps

1. **Incremental scrape** â€“ use LASTÂ MODIFIED header / match kickoff time to only hit fresh games.
2. **OptaÂ event enrichment** â€“ calculate inâ€‘play KPIs (xThreat, PPDA) directly in DB.
3. **Switch DB** â€“ liftâ€‘andâ€‘shift from SQLite to Postgres once data >2â€¯GB.
4. Containerise with Docker Compose: Scraper â€‘> DB â€‘> Metabase dashboard.

---

> **Milestones**
> *M1* â€“ Fixture scraper returns clean ID listÂ âœ“
> *M2* â€“ Match pipeline fills SQLite for one seasonÂ âœ“
> *M3* â€“ Automated nightly job + quality alerts via SlackÂ ğŸ
